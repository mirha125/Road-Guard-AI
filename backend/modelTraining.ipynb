{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13851487,"sourceType":"datasetVersion","datasetId":8823333},{"sourceId":14067138,"sourceType":"datasetVersion","datasetId":8953890},{"sourceId":14067431,"sourceType":"datasetVersion","datasetId":8954115}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport cv2\nimport numpy as np\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\nfrom tensorflow.keras import layers, models\n\n# ========================================================\n# 1ï¸âƒ£ CNN + LSTM Model\n# ========================================================\ndef build_cnn_lstm_model(sequence_length=16, input_shape=(224, 224, 3)):\n    cnn_base = tf.keras.applications.ResNet50(\n        include_top=False, weights='imagenet', pooling='avg', input_shape=input_shape\n    )\n    cnn_base.trainable = False\n\n    model = models.Sequential([\n        layers.TimeDistributed(cnn_base, input_shape=(sequence_length,) + input_shape),\n        layers.LSTM(256, return_sequences=False),\n        layers.Dense(128, activation='relu'),\n        layers.Dropout(0.5),\n        layers.Dense(2, activation='softmax')\n    ])\n\n    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n    return model\n\n# ========================================================\n# 2ï¸âƒ£ Video & Image Data Generator\n# ========================================================\nclass VideoImageDataGenerator(tf.keras.utils.Sequence):\n    def __init__(self, files, labels, sequence_length=16, batch_size=4, resize=(224,224)):\n        self.files = files\n        self.labels = labels\n        self.sequence_length = sequence_length\n        self.batch_size = batch_size\n        self.resize = resize\n\n    def __len__(self):\n        return int(np.ceil(len(self.files) / self.batch_size))\n\n    def __getitem__(self, idx):\n        batch_files = self.files[idx * self.batch_size:(idx + 1) * self.batch_size]\n        batch_labels = self.labels[idx * self.batch_size:(idx + 1) * self.batch_size]\n\n        X, y = [], []\n        for file, label in zip(batch_files, batch_labels):\n            frames = self._load_file(file)\n            X.append(frames)\n            y.append(label)\n\n        return np.array(X, dtype=np.float16), tf.keras.utils.to_categorical(y, num_classes=2)\n\n    def _load_file(self, file_path):\n        ext = os.path.splitext(file_path)[1].lower()\n        if ext == \".mp4\":\n            cap = cv2.VideoCapture(file_path)\n            frames = []\n            success, frame = cap.read()\n            while success and len(frames) < self.sequence_length:\n                frame = cv2.resize(frame, self.resize)\n                frames.append(frame)\n                success, frame = cap.read()\n            cap.release()\n            # Repeat last frame if video shorter than sequence_length\n            while len(frames) < self.sequence_length:\n                frames.append(frames[-1])\n            return np.array(frames, dtype=np.float16)\n\n        elif ext in [\".jpg\", \".png\"]:\n            frame = cv2.imread(file_path)\n            frame = cv2.resize(frame, self.resize)\n            # Repeat image to make sequence\n            frames = [frame] * self.sequence_length\n            return np.array(frames, dtype=np.float16)\n        else:\n            raise ValueError(f\"Unsupported file type: {file_path}\")\n\n# ========================================================\n# 3ï¸âƒ£ Paths\n# ========================================================\ninput_accident = \"/kaggle/input/roadguard-ai-dataset/RoadGuard-Al/dataset/train/accident\"\ninput_normal = \"/kaggle/input/roadguard-ai-dataset/RoadGuard-Al/dataset/train/normal\"\n\nmodel_save_path = \"/kaggle/working/accident_detector_cnn_lstm.h5\"\n\n# ========================================================\n# 4ï¸âƒ£ Collect all files (videos + images)\n# ========================================================\ndef get_files(path):\n    return sorted([os.path.join(path, f) for f in os.listdir(path) if f.lower().endswith((\".mp4\",\".jpg\",\".png\"))])\n\naccident_files = get_files(input_accident)\nnormal_files = get_files(input_normal)\n\nall_files = accident_files + normal_files\nall_labels = [1]*len(accident_files) + [0]*len(normal_files)\n\nprint(f\"ğŸš‘ Accident files: {len(accident_files)}\")\nprint(f\"ğŸ›£ Normal files: {len(normal_files)}\")\n\n# ========================================================\n# 5ï¸âƒ£ Train / Val / Test split\n# ========================================================\ntrain_files, test_files, train_labels, test_labels = train_test_split(\n    all_files, all_labels, test_size=0.10, random_state=42\n)\ntrain_files, val_files, train_labels, val_labels = train_test_split(\n    train_files, train_labels, test_size=0.20, random_state=42\n)\n\nprint(f\"âœ” Train: {len(train_files)} | Val: {len(val_files)} | Test: {len(test_files)}\")\n\ntrain_gen = VideoImageDataGenerator(train_files, train_labels, batch_size=2)\nval_gen = VideoImageDataGenerator(val_files, val_labels, batch_size=2)\ntest_gen = VideoImageDataGenerator(test_files, test_labels, batch_size=2)\n\n# ========================================================\n# 6ï¸âƒ£ Build & Train Model\n# ========================================================\nprint(\"ğŸ§  Building model...\")\nmodel = build_cnn_lstm_model()\nmodel.summary()\n\nprint(\"ğŸš€ Training...\")\nhistory = model.fit(train_gen, validation_data=val_gen, epochs=25, verbose=1)\n\n# ========================================================\n# 7ï¸âƒ£ Evaluate Model\n# ========================================================\ntest_loss, test_acc = model.evaluate(test_gen)\nprint(f\"ğŸ”¥ Test Accuracy: {test_acc:.4f}\")\nprint(f\"ğŸ”¥ Test Loss: {test_loss:.4f}\")\n\n# ========================================================\n# 8ï¸âƒ£ Save Model\n# ========================================================\nmodel.save(model_save_path)\nprint(f\"ğŸ’¾ Model saved at: {model_save_path}\")\nprint(\"ğŸ‰ Training complete!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T17:08:02.939421Z","iopub.execute_input":"2025-11-24T17:08:02.939745Z","iopub.status.idle":"2025-11-24T17:35:26.829692Z","shell.execute_reply.started":"2025-11-24T17:08:02.93972Z","shell.execute_reply":"2025-11-24T17:35:26.828721Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport cv2\nimport numpy as np\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\nfrom tensorflow.keras import layers, models\n\n# ========================================================\n# ğŸ”¥ Data Augmentation Layer\n# ========================================================\ndata_augmentation = tf.keras.Sequential([\n    layers.RandomFlip(\"horizontal\"),\n    layers.RandomRotation(0.1),\n    layers.RandomBrightness(0.1),\n    layers.RandomContrast(0.1),\n])\n\n# ========================================================\n# 1ï¸âƒ£ CNN + LSTM Model\n# ========================================================\ndef build_cnn_lstm_model(sequence_length=16, input_shape=(224, 224, 3)):\n    cnn_base = tf.keras.applications.ResNet50(\n        include_top=False, weights='imagenet', pooling='avg', input_shape=input_shape\n    )\n    cnn_base.trainable = False  # Freeze initially\n\n    model = models.Sequential([\n        layers.TimeDistributed(data_augmentation, input_shape=(sequence_length,) + input_shape),\n        layers.TimeDistributed(cnn_base),\n        layers.LSTM(256, return_sequences=False),\n        layers.Dense(128, activation='relu'),\n        layers.Dropout(0.5),\n        layers.Dense(2, activation='softmax')\n    ])\n\n    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n    return model\n\n# ========================================================\n# 2ï¸âƒ£ Video & Image Data Generator\n# ========================================================\nclass VideoImageDataGenerator(tf.keras.utils.Sequence):\n    def __init__(self, files, labels, sequence_length=16, batch_size=4, resize=(224,224)):\n        self.files = files\n        self.labels = labels\n        self.sequence_length = sequence_length\n        self.batch_size = batch_size\n        self.resize = resize\n\n    def __len__(self):\n        return int(np.ceil(len(self.files) / self.batch_size))\n\n    def __getitem__(self, idx):\n        batch_files = self.files[idx * self.batch_size:(idx + 1) * self.batch_size]\n        batch_labels = self.labels[idx * self.batch_size:(idx + 1) * self.batch_size]\n\n        X, y = [], []\n        for file, label in zip(batch_files, batch_labels):\n            frames = self._load_file(file)\n            X.append(frames)\n            y.append(label)\n\n        return np.array(X, dtype=np.float16), tf.keras.utils.to_categorical(y, num_classes=2)\n\n    def _load_file(self, file_path):\n        ext = os.path.splitext(file_path)[1].lower()\n        if ext == \".mp4\":\n            cap = cv2.VideoCapture(file_path)\n            frames = []\n            success, frame = cap.read()\n            while success and len(frames) < self.sequence_length:\n                frame = cv2.resize(frame, self.resize)\n                frames.append(frame)\n                success, frame = cap.read()\n            cap.release()\n            while len(frames) < self.sequence_length:\n                frames.append(frames[-1])\n            return np.array(frames, dtype=np.float16)\n\n        elif ext in [\".jpg\", \".png\"]:\n            frame = cv2.imread(file_path)\n            frame = cv2.resize(frame, self.resize)\n            frames = [frame] * self.sequence_length\n            return np.array(frames, dtype=np.float16)\n\n        else:\n            raise ValueError(f\"Unsupported file type: {file_path}\")\n\n# ========================================================\n# 3ï¸âƒ£ Paths\n# ========================================================\ninput_accident = \"/kaggle/input/roadguard-ai-dataset/RoadGuard-Al/dataset/train/accident\"\ninput_normal = \"/kaggle/input/roadguard-ai-dataset/RoadGuard-Al/dataset/train/normal\"\n\n\n\n# ========================================================\n# 4ï¸âƒ£ Collect all files\n# ========================================================\ndef get_files(path):\n    return sorted([os.path.join(path, f) for f in os.listdir(path) if f.lower().endswith((\".mp4\",\".jpg\",\".png\"))])\n\naccident_files = get_files(input_accident)\nnormal_files = get_files(input_normal)\n\nall_files = accident_files + normal_files\nall_labels = [1]*len(accident_files) + [0]*len(normal_files)\n\nprint(f\"ğŸš‘ Accident files: {len(accident_files)}\")\nprint(f\"ğŸ›£ Normal files: {len(normal_files)}\")\n\n# ========================================================\n# 5ï¸âƒ£ Train / Val / Test split\n# ========================================================\ntrain_files, test_files, train_labels, test_labels = train_test_split(\n    all_files, all_labels, test_size=0.10, random_state=42\n)\ntrain_files, val_files, train_labels, val_labels = train_test_split(\n    train_files, train_labels, test_size=0.20, random_state=42\n)\n\nprint(f\"âœ” Train: {len(train_files)} | Val: {len(val_files)} | Test: {len(test_files)}\")\n\ntrain_gen = VideoImageDataGenerator(train_files, train_labels, batch_size=2)\nval_gen = VideoImageDataGenerator(val_files, val_labels, batch_size=2)\ntest_gen = VideoImageDataGenerator(test_files, test_labels, batch_size=2)\n\n# ========================================================\n# 6ï¸âƒ£ Build & Train Model\n# ========================================================\nprint(\"ğŸ§  Building model...\")\nmodel = build_cnn_lstm_model()\nmodel.summary()\n\nprint(\"ğŸš€ Training (Phase 1: Frozen CNN)...\")\nhistory = model.fit(train_gen, validation_data=val_gen, epochs=10, verbose=1)\n\n# ========================================================\n# ğŸ”¥ 6B â€” Fine-Tuning\n# ========================================================\nprint(\"ğŸ”§ Unfreezing top layers for fine-tuning...\")\n\ncnn_base = model.layers[1].layer\ncnn_base.trainable = True\n\n# freeze all but last 50 layers\nfor layer in cnn_base.layers[:-50]:\n    layer.trainable = False\n\nmodel.compile(\n    optimizer=tf.keras.optimizers.Adam(1e-5),\n    loss='categorical_crossentropy',\n    metrics=['accuracy']\n)\n\nprint(\"ğŸš€ Training (Phase 2: Fine-tuning)...\")\nhistory_finetune = model.fit(train_gen, validation_data=val_gen, epochs=10, verbose=1)\n\n# ========================================================\n# 7ï¸âƒ£ Evaluate Model\n# ========================================================\ntest_loss, test_acc = model.evaluate(test_gen)\nprint(f\"ğŸ”¥ Test Accuracy: {test_acc:.4f}\")\nprint(f\"ğŸ”¥ Test Loss: {test_loss:.4f}\")\n\n# ========================================================\n# 8ï¸âƒ£ Save Model\n# ========================================================\nmodel.save(\"/kaggle/input/model-output/accident_detector_cnn_lstm.keras\")\nprint(f\"ğŸ’¾ Model saved \")\nprint(\"ğŸ‰ Training complete!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-09T02:45:30.461479Z","iopub.execute_input":"2025-12-09T02:45:30.461780Z","iopub.status.idle":"2025-12-09T03:22:09.788316Z","shell.execute_reply.started":"2025-12-09T02:45:30.461749Z","shell.execute_reply":"2025-12-09T03:22:09.787318Z"}},"outputs":[{"name":"stdout","text":"ğŸš‘ Accident files: 286\nğŸ›£ Normal files: 292\nâœ” Train: 416 | Val: 104 | Test: 58\nğŸ§  Building model...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"sequential_5\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_5\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\nâ”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\nâ”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\nâ”‚ time_distributed_4              â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m,   â”‚             \u001b[38;5;34m0\u001b[0m â”‚\nâ”‚ (\u001b[38;5;33mTimeDistributed\u001b[0m)               â”‚ \u001b[38;5;34m3\u001b[0m)                     â”‚               â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ time_distributed_5              â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m2048\u001b[0m)       â”‚    \u001b[38;5;34m23,587,712\u001b[0m â”‚\nâ”‚ (\u001b[38;5;33mTimeDistributed\u001b[0m)               â”‚                        â”‚               â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ lstm_2 (\u001b[38;5;33mLSTM\u001b[0m)                   â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            â”‚     \u001b[38;5;34m2,360,320\u001b[0m â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            â”‚        \u001b[38;5;34m32,896\u001b[0m â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            â”‚             \u001b[38;5;34m0\u001b[0m â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dense_5 (\u001b[38;5;33mDense\u001b[0m)                 â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)              â”‚           \u001b[38;5;34m258\u001b[0m â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\nâ”ƒ<span style=\"font-weight: bold\"> Layer (type)                    </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape           </span>â”ƒ<span style=\"font-weight: bold\">       Param # </span>â”ƒ\nâ”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\nâ”‚ time_distributed_4              â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>,   â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\nâ”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)               â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                     â”‚               â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ time_distributed_5              â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)       â”‚    <span style=\"color: #00af00; text-decoration-color: #00af00\">23,587,712</span> â”‚\nâ”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)               â”‚                        â”‚               â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ lstm_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            â”‚     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,360,320</span> â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)              â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">258</span> â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m25,981,186\u001b[0m (99.11 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">25,981,186</span> (99.11 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,393,474\u001b[0m (9.13 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,393,474</span> (9.13 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m23,587,712\u001b[0m (89.98 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">23,587,712</span> (89.98 MB)\n</pre>\n"},"metadata":{}},{"name":"stdout","text":"ğŸš€ Training (Phase 1: Frozen CNN)...\nEpoch 1/10\n\u001b[1m208/208\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m204s\u001b[0m 498ms/step - accuracy: 0.5858 - loss: 0.7591 - val_accuracy: 0.5673 - val_loss: 0.6697\nEpoch 2/10\n\u001b[1m208/208\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 330ms/step - accuracy: 0.6538 - loss: 0.6387 - val_accuracy: 0.6827 - val_loss: 0.5960\nEpoch 3/10\n\u001b[1m208/208\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 328ms/step - accuracy: 0.7068 - loss: 0.5650 - val_accuracy: 0.6635 - val_loss: 0.6576\nEpoch 4/10\n\u001b[1m208/208\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 330ms/step - accuracy: 0.6827 - loss: 0.6048 - val_accuracy: 0.6731 - val_loss: 0.6659\nEpoch 5/10\n\u001b[1m208/208\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 328ms/step - accuracy: 0.7673 - loss: 0.5444 - val_accuracy: 0.7308 - val_loss: 0.5609\nEpoch 6/10\n\u001b[1m208/208\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 327ms/step - accuracy: 0.7413 - loss: 0.5274 - val_accuracy: 0.7115 - val_loss: 0.5669\nEpoch 7/10\n\u001b[1m208/208\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 327ms/step - accuracy: 0.7846 - loss: 0.4526 - val_accuracy: 0.7404 - val_loss: 0.5522\nEpoch 8/10\n\u001b[1m208/208\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 327ms/step - accuracy: 0.7787 - loss: 0.4552 - val_accuracy: 0.7115 - val_loss: 0.5589\nEpoch 9/10\n\u001b[1m208/208\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 328ms/step - accuracy: 0.8417 - loss: 0.4060 - val_accuracy: 0.6635 - val_loss: 0.6125\nEpoch 10/10\n\u001b[1m208/208\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 327ms/step - accuracy: 0.8054 - loss: 0.4257 - val_accuracy: 0.6731 - val_loss: 0.5883\nğŸ”§ Unfreezing top layers for fine-tuning...\nğŸš€ Training (Phase 2: Fine-tuning)...\nEpoch 1/10\n\u001b[1m208/208\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m300s\u001b[0m 745ms/step - accuracy: 0.6320 - loss: 0.8521 - val_accuracy: 0.7500 - val_loss: 0.5265\nEpoch 2/10\n\u001b[1m208/208\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m120s\u001b[0m 577ms/step - accuracy: 0.7989 - loss: 0.4740 - val_accuracy: 0.7404 - val_loss: 0.5578\nEpoch 3/10\n\u001b[1m208/208\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m119s\u001b[0m 573ms/step - accuracy: 0.8059 - loss: 0.4631 - val_accuracy: 0.7788 - val_loss: 0.4858\nEpoch 4/10\n\u001b[1m208/208\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m119s\u001b[0m 573ms/step - accuracy: 0.8758 - loss: 0.3565 - val_accuracy: 0.7885 - val_loss: 0.4680\nEpoch 5/10\n\u001b[1m208/208\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m119s\u001b[0m 572ms/step - accuracy: 0.8970 - loss: 0.2912 - val_accuracy: 0.7981 - val_loss: 0.4609\nEpoch 6/10\n\u001b[1m208/208\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m119s\u001b[0m 573ms/step - accuracy: 0.8856 - loss: 0.2991 - val_accuracy: 0.7404 - val_loss: 0.5124\nEpoch 7/10\n\u001b[1m208/208\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m119s\u001b[0m 572ms/step - accuracy: 0.8978 - loss: 0.2776 - val_accuracy: 0.8173 - val_loss: 0.4424\nEpoch 8/10\n\u001b[1m208/208\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m119s\u001b[0m 572ms/step - accuracy: 0.9231 - loss: 0.2016 - val_accuracy: 0.7885 - val_loss: 0.4638\nEpoch 9/10\n\u001b[1m208/208\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m119s\u001b[0m 572ms/step - accuracy: 0.9583 - loss: 0.1730 - val_accuracy: 0.8269 - val_loss: 0.4705\nEpoch 10/10\n\u001b[1m208/208\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m119s\u001b[0m 573ms/step - accuracy: 0.9624 - loss: 0.1371 - val_accuracy: 0.8173 - val_loss: 0.4515\n\u001b[1m29/29\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 243ms/step - accuracy: 0.8503 - loss: 0.3937\n","output_type":"stream"},{"name":"stderr","text":"WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n","output_type":"stream"},{"name":"stdout","text":"ğŸ”¥ Test Accuracy: 0.8966\nğŸ”¥ Test Loss: 0.2764\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_47/1017339280.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[0;31m# 8ï¸âƒ£ Save Model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[0;31m# ========================================================\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/kaggle/input/model-output/accident_detector_cnn_lstm.h5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"ğŸ’¾ Model saved \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ğŸ‰ Training complete!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, fs_strategy, fs_persist, fs_threshold, fs_page_size, page_buf_size, min_meta_keep, min_raw_keep, locking, alignment_threshold, alignment_interval, meta_block_size, **kwds)\u001b[0m\n\u001b[1;32m    562\u001b[0m                                  \u001b[0mfs_persist\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfs_persist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfs_threshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfs_threshold\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m                                  fs_page_size=fs_page_size)\n\u001b[0;32m--> 564\u001b[0;31m                 \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_fid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muserblock_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mswmr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mswmr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    565\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlibver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    242\u001b[0m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_EXCL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfcpl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_TRUNC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfcpl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'a'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0;31m# Open in append mode (read/write).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n","\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n","\u001b[0;32mh5py/h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.create\u001b[0;34m()\u001b[0m\n","\u001b[0;31mOSError\u001b[0m: [Errno 30] Unable to synchronously create file (unable to open file: name = '/kaggle/input/model-output/accident_detector_cnn_lstm.h5', errno = 30, error message = 'Read-only file system', flags = 13, o_flags = 242)"],"ename":"OSError","evalue":"[Errno 30] Unable to synchronously create file (unable to open file: name = '/kaggle/input/model-output/accident_detector_cnn_lstm.h5', errno = 30, error message = 'Read-only file system', flags = 13, o_flags = 242)","output_type":"error"}],"execution_count":4},{"cell_type":"code","source":"model_save_path = \"/kaggle/working/accident_detector_cnn_lstm.keras\"\nmodel.save(model_save_path)\nprint(f\"ğŸ’¾ Model saved successfully at: {model_save_path}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-09T03:32:47.088853Z","iopub.execute_input":"2025-12-09T03:32:47.089164Z","iopub.status.idle":"2025-12-09T03:32:48.515662Z","shell.execute_reply.started":"2025-12-09T03:32:47.089139Z","shell.execute_reply":"2025-12-09T03:32:48.514832Z"}},"outputs":[{"name":"stdout","text":"ğŸ’¾ Model saved successfully at: /kaggle/working/accident_detector_cnn_lstm.keras\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# =========================================================\n# 0ï¸âƒ£ Install YOLOv8\n# =========================================================\n!pip install ultralytics --quiet\n\n# =========================================================\n# 1ï¸âƒ£ Imports\n# =========================================================\nimport os\nimport cv2\nimport shutil\nfrom tqdm import tqdm\nfrom ultralytics import YOLO\n\n# =========================================================\n# 2ï¸âƒ£ Define Input Paths\n# =========================================================\ninput_accident = \"/kaggle/input/roadguard-ai-dataset/RoadGuard-Al/dataset/train/accident\"\ninput_normal = \"/kaggle/input/roadguard-ai-dataset/RoadGuard-Al/dataset/train/normal\"\n\noutput_dataset = \"/kaggle/working/roadguard_dataset\"\nimages_folder = os.path.join(output_dataset, \"images\")\nlabels_folder = os.path.join(output_dataset, \"labels\")\n\n# =========================================================\n# 3ï¸âƒ£ Create YOLO Dataset Structure\n# =========================================================\nfor split in [\"train\", \"val\", \"test\"]:\n    for cls in [\"accident\", \"normal\"]:\n        os.makedirs(os.path.join(images_folder, split, cls), exist_ok=True)\n        os.makedirs(os.path.join(labels_folder, split, cls), exist_ok=True)\n\n# =========================================================\n# 4ï¸âƒ£ Split function\n# =========================================================\nimport random\ndef split_files(files, train_ratio=0.7, val_ratio=0.2):\n    random.shuffle(files)\n    n_total = len(files)\n    n_train = int(train_ratio * n_total)\n    n_val = int(val_ratio * n_total)\n    train_files = files[:n_train]\n    val_files = files[n_train:n_train+n_val]\n    test_files = files[n_train+n_val:]\n    return train_files, val_files, test_files\n\n# =========================================================\n# 5ï¸âƒ£ Helper: Convert videos to frames\n# =========================================================\ndef video_to_frames(video_path, output_folder, resize=(640, 640)):\n    os.makedirs(output_folder, exist_ok=True)\n    cap = cv2.VideoCapture(video_path)\n    count = 0\n    success, frame = cap.read()\n    while success:\n        frame_resized = cv2.resize(frame, resize)\n        frame_name = os.path.join(output_folder, f\"frame_{count:04d}.jpg\")\n        cv2.imwrite(frame_name, frame_resized)\n        success, frame = cap.read()\n        count += 1\n    cap.release()\n    return count\n\n# =========================================================\n# 6ï¸âƒ£ Collect all files (images + videos)\n# =========================================================\ndef get_files(path):\n    return [os.path.join(path, f) for f in os.listdir(path) if f.lower().endswith((\".jpg\", \".png\", \".mp4\"))]\n\naccident_files = get_files(input_accident)\nnormal_files = get_files(input_normal)\n\n# =========================================================\n# 7ï¸âƒ£ Split files into train/val/test\n# =========================================================\nacc_train, acc_val, acc_test = split_files(accident_files)\nnorm_train, norm_val, norm_test = split_files(normal_files)\n\nsplits = {\n    \"train\": {\"accident\": acc_train, \"normal\": norm_train},\n    \"val\": {\"accident\": acc_val, \"normal\": norm_val},\n    \"test\": {\"accident\": acc_test, \"normal\": norm_test}\n}\n\n# =========================================================\n# 8ï¸âƒ£ Copy images & convert videos\n# =========================================================\nfor split in [\"train\", \"val\", \"test\"]:\n    for cls in [\"accident\", \"normal\"]:\n        files = splits[split][cls]\n        for f in tqdm(files, desc=f\"{split} {cls}\"):\n            ext = os.path.splitext(f)[1].lower()\n            dest_img_folder = os.path.join(images_folder, split, cls)\n            if ext in [\".jpg\", \".png\"]:\n                shutil.copy(f, dest_img_folder)\n            elif ext == \".mp4\":\n                video_to_frames(f, dest_img_folder)\n            else:\n                print(f\"Unsupported file type: {f}\")\n\nprint(\"âœ… All images & frames ready!\")\n\n# =========================================================\n# 9ï¸âƒ£ Generate dummy YOLO labels (bounding boxes covering full image)\n# =========================================================\n# YOLO format: class x_center y_center width height (all normalized)\ndef generate_yolo_labels(image_folder, label_folder, class_id=0):\n    os.makedirs(label_folder, exist_ok=True)\n    for img_file in os.listdir(image_folder):\n        if not img_file.lower().endswith((\".jpg\", \".png\")):\n            continue\n        # YOLO: full image box\n        label_file = os.path.join(label_folder, img_file.replace(\".jpg\", \".txt\").replace(\".png\", \".txt\"))\n        with open(label_file, \"w\") as f:\n            f.write(f\"{class_id} 0.5 0.5 1.0 1.0\\n\")  # full image box\n\n# Generate labels for all splits/classes\nfor split in [\"train\", \"val\", \"test\"]:\n    for cls_idx, cls in enumerate([\"accident\", \"normal\"]):\n        img_folder = os.path.join(images_folder, split, cls)\n        lbl_folder = os.path.join(labels_folder, split, cls)\n        generate_yolo_labels(img_folder, lbl_folder, class_id=cls_idx)\n\nprint(\"âœ… YOLO labels generated!\")\n\n# =========================================================\n# ğŸ”Ÿ Prepare YOLOv8 dataset YAML\n# =========================================================\ndataset_yaml_path = os.path.join(output_dataset, \"roadguard_dataset.yaml\")\nwith open(dataset_yaml_path, \"w\") as f:\n    f.write(f\"train: {images_folder}/train\\n\")\n    f.write(f\"val: {images_folder}/val\\n\")\n    f.write(f\"test: {images_folder}/test\\n\")\n    f.write(\"nc: 2\\n\")\n    f.write(\"names: ['accident','normal']\\n\")\n\nprint(f\"âœ… Dataset YAML created: {dataset_yaml_path}\")\n\n# =========================================================\n# 1ï¸âƒ£1ï¸âƒ£ Train YOLOv8\n# =========================================================\nmodel = YOLO(\"yolov8n.pt\")  # nano model pretrained\n\nmodel.train(\n    data=dataset_yaml_path,\n    imgsz=640,\n    epochs=2,\n    batch=4,\n    workers=2,\n    project=\"/kaggle/working/yolov8_accident_detector\",\n    name=\"exp1\",\n    exist_ok=True\n)\n\n# =========================================================\n# 1ï¸âƒ£2ï¸âƒ£ Inference Example\n# =========================================================\n# Load best model\nbest_model_path = \"/kaggle/working/yolov8_accident_detector/exp1/weights/best.pt\"\nmodel = YOLO(best_model_path)\n\n# Predict on a test image\ntest_img = os.path.join(images_folder, \"test\", \"accident\", os.listdir(os.path.join(images_folder, \"test\", \"accident\"))[0])\nresults = model.predict(test_img, conf=0.25, save=True)\nprint(\"âœ… Inference done, output saved in 'runs/detect' folder\")\n\n# Predict on a test video (optional)\n# test_video = \"/kaggle/input/roadguard-ai-dataset/RoadGuard-Al/dataset/train/accident/video1.mp4\"\n# results = model.predict(test_video, conf=0.25, save=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-09T06:22:52.850404Z","iopub.execute_input":"2025-12-09T06:22:52.850686Z"}},"outputs":[{"name":"stderr","text":"train accident:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 76/200 [12:57<42:13, 20.43s/it]  ","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"import os\nimport cv2\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.applications.resnet50 import preprocess_input\n# ======================================================\n# 1ï¸âƒ£ Load the trained CNN+LSTM model\n# ======================================================\nmodel_path = \"/kaggle/working/accident_detector_cnn_lstm.keras\"\nmodel = tf.keras.models.load_model(model_path, compile=False)\nprint(\"âœ… Model loaded successfully!\")\n\n# ======================================================\n# 2ï¸âƒ£ Access the TimeDistributed CNN (ResNet50)\n# ======================================================\ncnn_td_layer = model.layers[1]  # TimeDistributed(ResNet50)\ncnn_backbone = cnn_td_layer.layer\nprint(\"CNN Backbone:\", cnn_backbone.name)\n\n# ======================================================\n# 3ï¸âƒ£ Find last convolutional layer\n# ======================================================\nlast_conv_layer_name = None\nfor layer in reversed(cnn_backbone.layers):\n    if isinstance(layer, tf.keras.layers.Conv2D):\n        last_conv_layer_name = layer.name\n        break\nprint(\"Last Conv Layer:\", last_conv_layer_name)\n\n# ======================================================\n# 4ï¸âƒ£ Build Grad-CAM model for single frame\n# ======================================================\n# Build CNN with dummy input so outputs are defined\ndummy_input = tf.random.normal((1, 224, 224, 3))\ncnn_backbone(dummy_input)\n\ngrad_model = tf.keras.models.Model(\n    inputs=cnn_backbone.input,\n    outputs=[\n        cnn_backbone.get_layer(last_conv_layer_name).output,\n        cnn_backbone.output\n    ]\n)\nprint(\"âœ… Grad-CAM model ready!\")\n\n# ======================================================\n# 5ï¸âƒ£ Grad-CAM function for a single frame\n# ======================================================\ndef generate_gradcam_frame(frame, class_index):\n    # Preprocess the frame for ResNet50\n    frame_input = np.float32(frame)\n    frame_input = preprocess_input(frame_input)\n    img_tensor = tf.expand_dims(frame_input, axis=0)  # (1, H, W, 3)\n\n    with tf.GradientTape() as tape:\n        conv_outputs, predictions = grad_model(img_tensor)\n        loss = predictions[:, class_index]\n\n    grads = tape.gradient(loss, conv_outputs)\n    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n    conv_outputs = conv_outputs[0]\n\n    heatmap = tf.reduce_sum(tf.multiply(pooled_grads, conv_outputs), axis=-1)\n    heatmap = tf.maximum(heatmap, 0) / (tf.reduce_max(heatmap) + 1e-10)\n    heatmap = cv2.resize(heatmap.numpy(), (frame.shape[1], frame.shape[0]))\n    heatmap = np.uint8(255 * heatmap)\n    heatmap_color = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n\n    # Original frame in BGR for blending\n    output = cv2.addWeighted(cv2.cvtColor(frame, cv2.COLOR_RGB2BGR), 0.7, heatmap_color, 0.5, 0)\n    return output\n\n# ======================================================\n# 6ï¸âƒ£ Grad-CAM for a sequence of frames\n# ======================================================\ndef generate_gradcam_sequence(sequence, class_index, save_path):\n    os.makedirs(save_path, exist_ok=True)\n    for i, frame in enumerate(sequence):\n        gradcam = generate_gradcam_frame(frame, class_index)\n        cv2.imwrite(f\"{save_path}/frame_{i}.jpg\", gradcam)\n    print(f\"âœ… Saved {len(sequence)} Grad-CAM frames to: {save_path}\")\n\n# ======================================================\n# 7ï¸âƒ£ Example: Run Grad-CAM on your dataset\n# ======================================================\n# Suppose you have a list of video/image sequences\n# Each sequence is (sequence_length, 224, 224, 3)\n# For demonstration, using a dummy sequence:\nsequence_length = 16\ndummy_sequence = np.random.randint(0, 255, (sequence_length, 224, 224, 3), dtype=np.uint8)\n\n# Predict class for sequence using your full model\npred = model.predict(np.expand_dims(dummy_sequence, axis=0))  # (1, 2)\nclass_index = np.argmax(pred)\n\n# Output folder\noutput_folder = \"/kaggle/working/gradcam_output\"\ngenerate_gradcam_sequence(dummy_sequence, class_index, save_path=output_folder)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}